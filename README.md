# COMM4190 Spring 2025 - Research Project

# PROJECT DESCRIPTION AND OVERVIEW

## LLMs as Emotional Counselors: Analyzing AI Communication in Sensitive Health Contexts

This research examines how large language models (LLMs) function as informal mental health resources. As AI systems become increasingly accessible, people turn to them with deeply personal health concerns—from depression and grief to medical questions. This shift raises important questions about how these models communicate sensitive information and the implications for user wellbeing.

### Project Scope

This study evaluates how ChatGPT-4 and Claude-3-Opus respond to emotionally charged health queries across six domains:
- Mental health disclosure
- Reproductive loss
- Diagnostic requests
- Chronic illness despair
- Bereavement
- Health misinformation

Responses are analyzed based on tone, clarity, disclaimer use, and ethical boundaries to identify patterns in AI-human communication during vulnerable moments.

### Research Motivation

This research was motivated by:
1. Increasing user reliance on AI for emotional support
2. Ethical concerns about clinical boundaries 
3. Questions about trust dynamics when AI simulates empathy
4. Inconsistent safeguards across platforms

### Significance

This project contributes to AI ethics in healthcare by:
- Documenting how current LLMs handle emotionally charged health scenarios
- Identifying specific communication patterns across different models
- Highlighting risks associated with different approaches to emotional design
- Offering recommendations for responsible development

By understanding how LLMs communicate in these high-stakes contexts, we can develop design approaches that balance emotional support with appropriate ethical boundaries—ensuring AI systems enhance rather than complicate human wellbeing.